\documentclass[parskip=half]{scrartcl}

\usepackage[hidelinks]{hyperref}

\renewcommand*{\thefootnote}{(\arabic{footnote})}

\usepackage{microtype}
\usepackage{fontspec}
\usepackage{unicode-math}

\setmainfont{Georgia}
\setsansfont{Helvetica Neue}
\setmathfont{Stix Two Math}

\KOMAoptions{DIV=calc}

\begin{document}

\input{../author}

\begin{center}
    \Large
    \textsf{\textbf{Differential Privacy}}
        
    \vspace{0.4cm}
    \large
    Homework 5: Privacy Preserving Technologies
        
    \vspace{0.4cm}
    \docauthor{}
       
    \vspace{0.9cm}
\end{center}

\tableofcontents

\section{Differential privacy of query Q1}

\subsection{Choosing the \texorpdfstring{$\epsilon$}{ε}}

\begin{enumerate}
    \item \textit{What is the global sensitivity of the query Q1?}
    
    For Q1, all votes for all candidates are counted, and the total of votes
    for each candidate is returned. As such, any vote by a voter will increment
    the total of votes for one candidate by one (assuming a single-vote
    election), regardless of who they vote for.

    We can thus say that for Q1, the difference of vote counts per candidate
    for two adjacent tables is $1$, and hence the global sensitivity is
    $\Delta q_1 = 1$.
    
    \item \textit{Which $\epsilon$ should be taken so that the probability of
    getting the correct result is at least $0.9$?}

    We wish that the probability of getting the correct election outcome be at
    least $0.9$. This is only the case where all three vote totals are correct.
    As such, each vote total must be correct with probability at least
    $\sqrt[3]{0.9}$, assuming that the noise distribution is the same for all
    three candidates. Conversely, this means that we do not get the correct
    election outcome should we get a wrong vote total for any candidate, and
    the probability of this happening, i.e., the probability of failure, must
    be smaller than $\beta = 1-\sqrt[3]{0.9}$.

    A difference of even one vote from the true vote count of a candidate can
    change the outcome of the election. Because we round noise-added values to
    the nearest integer value, and values themselves are already integers, we
    require noise to be below $0.5$ to ensure the ``noised'' value is
    rounded down. This avoids adding one vote to the summed candidate vote
    count, and is essentially equivalent of applying no noise.
    
    More specifically, in the given case, rounding the noise to the nearest
    integer before adding it to the vote count is equivalent to rounding the
    result, and hence does not go against the post-processing theorem. We can
    hence say that $|\textit{noise}| < |\alpha| = 0.5$ must hold with
    probability of at least $\beta$\footnotemark{}, where $\alpha$ is the
    accuracy.

    \footnotetext{$\beta$ is an exclusive upper bound for the error, as
    $1-\sqrt[3]{0.9}$ is included in the probability of success.}

    We observe that since Q1 releases information about all three candidates,
    but a voter can only vote for one candidate, the total vote counts for each
    candidate are disjoint subsets with regards to the vote of a single voter.
    We then decompose the query Q1 into three sub-queries consisting of the
    vote count for each candidate. We hence have $Q1_f, Q1_w, Q1_b$, with
    $Q1_f \cup Q1_w \cup Q1_b = Q1$, and $Q1_x \cap Q1_y = \varnothing$ for
    $x, y\in, \{f, w, b\}$, and $x\neq y$.

    If we consider the noise distribution to be the same for all three
    candidates, we have that $Q1_f, Q1_w$ and $Q1_b$ provide equivalent
    $\epsilon$-differential privacy, which is also the total privacy level of
    Q1 due to parallel composition.

    We have
    $$
        \alpha = \frac{\Delta q_1}{\epsilon}\ln{\frac{1}{\beta}}
        \quad\Leftrightarrow\quad
        -\frac{\alpha\epsilon}{\Delta q_1} = \ln{\beta}
        \quad\Leftrightarrow\quad
        \beta = e^{-\frac{\alpha\epsilon}{\Delta q_1}}.
    $$
    
    Thus, if we want $\beta < 1 - \sqrt[3]{0.9}$ with $\Delta q_1 = 1$, we have
    \begin{alignat*}{2}
        \quad &&
            1 - \sqrt[3]{0.9} &> e^{-\frac{\alpha\epsilon}{\Delta q_1}}\\
        \Leftrightarrow\quad &&
            \ln{\left(1 - \sqrt[3]{0.9}\right)} &> -\alpha\epsilon\\
        \Leftrightarrow\quad &&
            \epsilon &> -2 \ln{\left(1 - \sqrt[3]{0.9}\right)} \approx 6.733.
    \end{alignat*}

    To get the correct result with probability at least $0.9$, we must take
    $\epsilon$ of at least $6.733$\footnote{$6.733$ is already large enough
    because we rounded up.}.
\end{enumerate}

\subsection{Dealing with negative counts}

\textit{Which of the following modifications would keep the system
$\epsilon$-differentially private with the same value of $\epsilon$ due to the
post-processing theorem? Why?}

\begin{enumerate}
    \item \textit{Sample only positive noise, i.e. from the interval
    $[0, \infty)$.}

    According to the post-processing theorem, processing must not depend on the
    noise and/or private data added by the mechanism. Thus, this case does not
    fall under the post-processing theorem, and the modified system may no
    longer be $\epsilon$-differentially private, as it may leak information
    about the underlying data. More specifically, we have altered the noise
    distribution and introduced value bias.

    \item \textit{If the true vote count of some candidate is some value $y$,
    sample the noise from the interval $[-y, \infty)$.}

    The vote count of some candidate is private data, i.e., data we seek to
    protect. The post-processing theorem states that processing must not depend
    on private data. Hence, this system may no longer be
    $\epsilon$-differentially private, as it may leak information about the
    underlying data. More specifically, we have altered the noise distribution
    and introduced value bias.

    \item \textit{If the noisy vote count of some candidate is negative, then
    re-sample the noise until the result becomes positive.}
    
    The approach may reveal timing information exploitable by side-channel
    attacks for an active adversary. We have also altered the noise
    distribution, as our result is equivalent to once more applying noise only
    from the $[-y, \infty)$ interval. The system is therefore
    not $\epsilon$-differentially private.

    We note that we understand this modification to re-add the noise to the
    original value. The situation would be different if noise was added to
    already noised output.

    \item \textit{If the noisy vote count of some candidate is negative, then
    round this negative count up to $0$.}

    This modification keeps the system $\epsilon$-differentially private. This
    processing is deterministic, and can be done with anyone having access to
    the $\epsilon$-differentially private output of the initial mechanism.
\end{enumerate}

\section{Differential privacy for query Q2}

\subsection{Choosing the \texorpdfstring{$\epsilon$}{ε}}

\begin{enumerate}
    \item \textit{Which $\epsilon$ should be taken so that the probability of
    getting a correct result should be at least $0.9$?}

    For a score function, we estimate accuracy with
    $$
        \Pr[OPT - Q2(t, y^*) \ge \alpha] \le \beta,
    $$
    where $OPT=Q2(t, q_2(t))$ is the score of the optimal solution, and $y^*$ is
    the output of the exponential mechanism.
    
    More particularly, for any $m$, we have
    $$
        \Pr[OPT - Q2(t, y^*) \ge
        \frac{2\Delta q_2}{\epsilon}(\ln{(|Y|)} + m)] \le e^{-m},
    $$
    where $Y = \{Fox, Wolf, Bear\}$, and its cardinality is $|Y|=3$.

    We see that $\beta = e^{-m}$ and that $\alpha =
    \frac{2\Delta q_2}{\epsilon}(\ln{(|Y|)} + m)$. But we also have that
    $\beta < 1 - 0.9$ since the probability of getting a correct result must be
    $\ge 0.9$.

    We can recover $m$ with
    $$
        \beta = e^{-m} \quad\Leftrightarrow\quad
        \ln{\beta} = -m \quad\Leftrightarrow\quad
        m = -\ln{\beta} = -\ln{(1 - 0.9)}=\ln{10}.
    $$

    Additionally, we want to achieve an accuracy $<1$, so that the
    difference from the true count must be below one. This is because a
    difference of one or more will alter the correct vote count, and may hence
    change the election result. We then have an upper bound $\alpha=1$.

    To calculate $\epsilon$ we must determine the global sensitivity of the
    score function. Here also, the difference of vote counts per candidate
    for two adjacent tables is $1$, and since the score function takes maximum
    value of the sums of all votes for each candidate, the optimal score can
    be affected by at most $1$. The global sensitivity is hence
    $\Delta q_2 = 1$.

    For $\alpha = 1$, $m=\ln{10}$, and $\Delta q_2=1$, we then have
    \begin{alignat*}{2}
        \quad&& \alpha &= \frac{2\Delta q_2}{\epsilon}(\ln(|Y|) + m)\\
        \Leftrightarrow\quad&&
        \epsilon &= \frac{2}{\alpha}(\ln(3) + \ln(10))\\
        \Leftrightarrow\quad&&
        \epsilon &= 2 \ln{30} \approx 6.802.
    \end{alignat*}

    To achieve a probability of at least $0.9$ of getting a correct result, we
    must pick an $\epsilon$ of at least $6.802$.

    \item \textit{How likely will the system output the actual winner for this
    particular dataset?}

    From the graph, we see that the wolf has the best score, i.e., the highest
    vote count, with $67$ votes in total.

    We have for $y = \textit{Wolf}$ that
    \begin{align*}
        \Pr[\textit{Exp}(t, Y, Q2, \epsilon) = y] &=
        \frac{e^\frac{\epsilon Q2(t, y)}{2\Delta q_2}}
        {\sum_{y'\in Y} e^\frac{\epsilon Q2(t, y')}{2\Delta q_2}}
        =
        \frac{e^{Q2(t, y) \cdot \ln{30}}}
        {\sum_{y'\in Y} e^{Q2(t, y') \cdot \ln{30}}}\\
        &=
        \frac{e^{67\ln{30}}}
        {
            e^{67\ln{30}} +
            e^{41\ln{30}} +
            e^{25\ln{30}}
        }\\
        &\approx 0.999999999999999999999999999999999999996.
    \end{align*}
    \textit{\footnotesize(the computation can be viewed on
    WolframAlpha\footnotemark{})}

    We see that for $\epsilon\approx 6.802$, it is essentially certain that
    the system will output the actual winner.

    \footnotetext{\url{https://tinyurl.com/diffppt-212} (shortened for
    display)}

\end{enumerate}

\subsection{The goodness of \texorpdfstring{$\epsilon$}{ε}}

\begin{enumerate}
    \item \textit{The initial motivation behind differential privacy was the
    case of Fox coercing a group of 10 rabbits. Which $\epsilon$-DP is actually
    guaranteed for the group of $10$ rabbits?}

    To obtain $\epsilon$-DP for a group of size $k$, we can use any
    $\frac{\epsilon}{k}$-DP mechanism. Currently, the $\epsilon$-DP applies to
    $1$-adjacent tables, i.e., a group of size $1$. As such, we have that
    $\epsilon = \frac{\epsilon'}{10}$, and for a group of $10$ rabbits, we
    actually only have $10\epsilon$-DP, which is far weaker.

    For a group of $10$ rabbits, an approximately $68$-DP is guaranteed, which
    is a weak guarantee.

    \item \textit{In the definition of $\epsilon$-DP, the inequality
    $\Pr[\mathcal{M}_q(t) = y] \le e^\epsilon \cdot \Pr[\mathcal{M}_q(t') = y]$
    holds for any neighbouring datasets $t$ and $t'$, and any noisy output $y$.
    The query Q2 uses exponential mechanism with three possible outputs $y \in
    \{Fox, Wolf, Bear\}$. Show that, for any choice of $t$ and $t'$, the
    previously obtained $\epsilon$ does not give any privacy guarantees for at
    least one output $y$.}

    We have
    \begin{alignat*}{2}
        \quad&&
        \Pr[\mathcal{M}_q(t) = y] &\le
        e^\epsilon \cdot \Pr[\mathcal{M}_q(t') = y]\\
        \Leftrightarrow\quad&&
        \frac{\Pr[\mathcal{M}_q(t) = y]}
        {\Pr[\mathcal{M}_q(t') = y]} &\le e^\epsilon,
    \end{alignat*}
    and for $\epsilon = 2\ln{30}$, and $\Delta q_2=1$ we have
    \begin{alignat*}{2}
        \quad&&
        \Pr[\mathcal{M}_q(t) = y] &\le
        e^\epsilon \cdot \Pr[\mathcal{M}_q(t') = y]\\
        \Leftrightarrow\quad&&
        \Pr[\mathcal{M}_q(t) = y] &\le
        e^\epsilon \cdot
        \frac{
            e^{\frac{\epsilon Q2(t', y)}{2\Delta q_2}}
        }{
            \sum_{y'\in Y} e^{\frac{\epsilon Q2(t', y')}{2\Delta q_2}}
        }\\
        \Leftrightarrow\quad&&
        \Pr[\mathcal{M}_q(t) = y] &\le
        \frac{
            e^{\epsilon + \frac{\epsilon Q2(t', y)}{2\Delta q_2}}
        }{
            \sum_{y'\in Y} e^{\frac{\epsilon Q2(t', y')}{2\Delta q_2}}
        }\\
        \Leftrightarrow\quad&&
        \Pr[\mathcal{M}_q(t) = y] &\le
        \frac{
            e^{(2 + Q2(t', y)) \cdot \ln{30}}
        }{
            \sum_{y'\in Y} e^{Q2(t', y') \cdot \ln{30}}
        }.
    \end{alignat*}
    
    For $y=\textit{Wolf}$ with one vote removed, i.e., $Q2(t', y)=66$, we
    hence have that
    $$
        \Pr[\mathcal{M}_q(t) = y] \le
        \frac{
            e^{(2 + 66) \cdot \ln{30}}
        }{
            e^{66\ln{30}} +
            e^{41\ln{30}} +
            e^{25\ln{30}}
        } \approx 900.
    $$

    Because $\Pr[\mathcal{M}_q(t) = y] \le 1 < 900$ is always true and a
    trivial bound, there is at least one output $y$ (we showed for the wolf),
    where the previously obtained $\epsilon$ does not give any privacy
    guarantees.

    \item \textit{After looking at the numbers in the previous task, the
    programmer decided that, for a reasonable privacy guarantee, we need to
    take $10$ times smaller $\epsilon$. This will inevitably affect the
    accuracy. Are there now reasonable guarantees for the probability that the
    noisy result is correct?}

    Let $\epsilon'=\frac{\epsilon}{10}$. We have for $y=\textit{Wolf}$ that
    \begin{align*}
        \Pr[\textit{Exp}(t, Y, Q2, \epsilon') = y]
        &=
        \frac{
            e^\frac{\epsilon' Q2(t, y)}{2\Delta q_2}
        }{
            \sum_{y'\in Y}
            e^\frac{\epsilon' Q2(t, y')}{2\Delta q_2}
        }=
        \frac{
            e^{\frac{\ln{30}}{10} Q2(t, y)}
        }{
            \sum_{y'\in Y}
            e^{\frac{\ln{30}}{10} Q2(t, y')}
        }\\
        &=
        \frac{
            e^{\frac{\ln{30}}{10} \cdot 67}
        }{
            e^{\frac{\ln{30}}{10} \cdot 67} +
            e^{\frac{\ln{30}}{10} \cdot 41} +
            e^{\frac{\ln{30}}{10} \cdot 25}
        }\\
        &\approx 0.99986.
    \end{align*}

    We see that while the accuracy is lessened, it is still accurate with high
    probability ($\ge 0.9$), so there are still reasonable guarantees regarding
    the correctness of the result.

    \item \textit{In the definition of $\epsilon$-DP, the inequality
    $\Pr[\mathcal{M}_q(t) = y] \le e^\epsilon \cdot \Pr[\mathcal{M}_q(t') = y]$
    provides a guaranteed upper bound, while in practice
    $\Pr[\mathcal{M}_q(t) = y]$ and $\Pr[\mathcal{M}_q(t') = y]$ can be even
    closer to each other. How do you think, did the programmer overreact by
    taking a smaller $\epsilon$?}

    Here we have that the candidate results are fairly different from
    each-other. That is, the first and second candidate have $26$ votes as a
    difference. We need not concern ourself with the difference of second and
    third place, since for Q2, only the winner matters. As such, reducing
    $\epsilon$ was not an issue due to the large difference. Reducing
    $\epsilon$ so drastically had the vote differences been small, e.g., only
    one or two votes, then the programmer could have affected the outcome of
    the election.

    Therefore, whether the programmer overreacted depends on whether they knew
    the magnitude differences between the counts. I do believe that reducing
    $\epsilon$ so drastically was an overreaction if no previous thought was
    put into this decision by the programmer, particularly considering the
    stakes.
\end{enumerate}

\section{Decision}

\textit{Would you prefer Q1 (using Laplace mechanism with rounding) or Q2
(using exponential mechanism), and with which parameters, if you were}
\begin{enumerate}
    \item \textit{as an election organizer (interested in a fair election)?}
    
    As an election organiser, Q1 is very important. This is because the winning
    party is not the only important aspect of an election. Fairness must apply
    to all parties, and it may be that there was foul play regarding candidates
    with less votes, which prevented them for getting as many votes as they
    should have (e.g., anomaly of a system). As an organiser, I wish to get
    accuracy for all candidates, not just the winner.

    Therefore, Q1 with $6.7$-DP more or less guarantees a fair election, at
    some privacy cost for voters. Still, guessing how a voter votes can be
    profiled in so many ways, that the Q1 mechanism does not seem to add much
    more privacy risks in an election context. Therefore, the weaker privacy is
    acceptable to preserve the integrity and fairness of the election.

    \item \textit{as a voter (interested in privacy)?}
    
    As a voter, I would prefer Q2, as it allows getting exceptional accuracy
    (for the main result) for decent privacy. For example, with $\epsilon = 
    \frac{2\ln{30}}{10} \approx 0.68$ we still get the correct election outcome
    with probability $\ge 0.99$ (for the given values). The election would
    remain fair, and there would be acceptable privacy for voters. We would
    then also have $6.8$-DP for the group of $10$ rabbits, which is reasonable.
\end{enumerate}

\end{document}
